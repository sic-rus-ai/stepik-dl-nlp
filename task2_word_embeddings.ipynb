{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "try:\n",
    "    BASE_PATH\n",
    "except NameError:\n",
    "    BASE_PATH = '.'\n",
    "\n",
    "MARKER = 'Need clone'\n",
    "GIT_STATUS = !cd {BASE_PATH} && git status -s 2>/dev/null || echo {MARKER}\n",
    "NEED_CLONE = GIT_STATUS and GIT_STATUS[-1] == MARKER\n",
    "\n",
    "if NEED_CLONE:  # we run code on Google Colab or Kaggle and have not installed required packages yet\n",
    "    !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
    "    BASE_PATH = './stepik-dl-nlp'\n",
    "else:\n",
    "    print('Repo is already cloned and packages are installed.')\n",
    "\n",
    "if BASE_PATH not in sys.path:\n",
    "    sys.path.append(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:30.785285Z",
     "start_time": "2019-10-29T19:19:29.542846Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlnlputils\n",
    "from dlnlputils.data import tokenize_corpus, build_vocabulary, texts_to_token_ids, \\\n",
    "    PaddedSequenceDataset, Embeddings\n",
    "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
    "from dlnlputils.visualization import plot_vectors\n",
    "\n",
    "init_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных и подготовка корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:31.270503Z",
     "start_time": "2019-10-29T19:19:30.787789Z"
    }
   },
   "outputs": [],
   "source": [
    "full_dataset = list(pd.read_csv(f'{BASE_PATH}/datasets/nyt-ingredients-snapshot-2015.csv')['input'].dropna())\n",
    "random.shuffle(full_dataset)\n",
    "\n",
    "TRAIN_VAL_SPLIT = int(len(full_dataset) * 0.7)\n",
    "train_source = full_dataset[:TRAIN_VAL_SPLIT]\n",
    "test_source = full_dataset[TRAIN_VAL_SPLIT:]\n",
    "print(\"Обучающая выборка\", len(train_source))\n",
    "print(\"Тестовая выборка\", len(test_source))\n",
    "print()\n",
    "print('\\n'.join(train_source[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:32.137838Z",
     "start_time": "2019-10-29T19:19:31.272363Z"
    }
   },
   "outputs": [],
   "source": [
    "# токенизируем\n",
    "train_tokenized = tokenize_corpus(train_source)\n",
    "test_tokenized = tokenize_corpus(test_source)\n",
    "print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:32.325205Z",
     "start_time": "2019-10-29T19:19:32.140837Z"
    }
   },
   "outputs": [],
   "source": [
    "# строим словарь\n",
    "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=0.9, min_count=5, pad_word='<PAD>')\n",
    "print(\"Размер словаря\", len(vocabulary))\n",
    "print(list(vocabulary.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:32.686258Z",
     "start_time": "2019-10-29T19:19:32.327711Z"
    }
   },
   "outputs": [],
   "source": [
    "# отображаем в номера токенов\n",
    "train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n",
    "test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n",
    "\n",
    "print('\\n'.join(' '.join(str(t) for t in sent)\n",
    "                for sent in train_token_ids[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:32.967989Z",
     "start_time": "2019-10-29T19:19:32.688319Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist([len(s) for s in train_token_ids], bins=20);\n",
    "plt.title('Гистограмма длин предложений');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:33.001487Z",
     "start_time": "2019-10-29T19:19:32.970153Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LEN = 20\n",
    "train_dataset = PaddedSequenceDataset(train_token_ids,\n",
    "                                      np.zeros(len(train_token_ids)),\n",
    "                                      out_len=MAX_SENTENCE_LEN)\n",
    "test_dataset = PaddedSequenceDataset(test_token_ids,\n",
    "                                     np.zeros(len(test_token_ids)),\n",
    "                                     out_len=MAX_SENTENCE_LEN)\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм обучения - Skip Gram Negative Sampling\n",
    "\n",
    "**Skip Gram** - предсказываем соседние слова по центральному слову\n",
    "\n",
    "**Negative Sampling** - аппроксимация softmax\n",
    "\n",
    "$$ W, D \\in \\mathbb{R}^{Vocab \\times EmbSize} $$\n",
    "\n",
    "$$ \\sum_{CenterW_i} P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) \\rightarrow \\max_{W,D} $$\n",
    "\n",
    "$$ P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) = \\prod_j P(CtxW_j | CenterW_i; W, D) $$\n",
    "    \n",
    "$$ P(CtxW_j | CenterW_i; W, D) = \\frac{e^{w_i \\cdot d_j}} { \\sum_{j=1}^{|V|} e^{w_i \\cdot d_j}} = softmax \\simeq \\frac{e^{w_i \\cdot d_j^+}} { \\sum_{j=1}^{k} e^{w_i \\cdot d_j^-}}, \\quad k \\ll |V| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:33.065376Z",
     "start_time": "2019-10-29T19:19:33.003081Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_diag_mask(size, radius):\n",
    "    \"\"\"Квадратная матрица размера Size x Size с двумя полосами ширины radius вдоль главной диагонали\"\"\"\n",
    "    idxs = torch.arange(size)\n",
    "    abs_idx_diff = (idxs.unsqueeze(0) - idxs.unsqueeze(1)).abs()\n",
    "    mask = ((abs_idx_diff <= radius) & (abs_idx_diff > 0)).float()\n",
    "    return mask\n",
    "\n",
    "make_diag_mask(10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative Sampling** работает следующим образом - мы **максимизируем сумму вероятностей двух событий**: \n",
    "\n",
    "* \"этот пример центрального слова вместе с контекстными словами взят **из тренировочной выборки**\": $$ P(y=1 | CenterW_i; CtxW_j) = sigmoid(w_i \\cdot d_j) = \\frac{1}{1+e^{-w_i \\cdot d_j}} $$\n",
    "\n",
    "$$ \\\\ $$\n",
    "\n",
    "* \"этот пример центрального слова вместе со случайми контекстными словами **выдуман** \": $$ P(y=0 | CenterW_i; CtxW_{noise}) = 1 - P(y=1 | CenterW_i;  CtxW_{noise}) = \\frac{1}{1+e^{w_i \\cdot d_{noise}}} $$\n",
    "\n",
    "$$ \\\\ $$\n",
    "\n",
    "$$ NEG(CtxW_j, CenterW_i) = log(\\frac{1}{1+e^{-w_i \\cdot d_j}}) + \\sum_{l=1}^{k}log(\\frac{1}{1+e^{w_i \\cdot d_{noise_l}}})  \\rightarrow \\max_{W,D} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:33.101379Z",
     "start_time": "2019-10-29T19:19:33.068154Z"
    }
   },
   "outputs": [],
   "source": [
    "class SkipGramNegativeSamplingTrainer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, sentence_len, radius=5, negative_samples_n=5):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.negative_samples_n = negative_samples_n\n",
    "\n",
    "        self.center_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n",
    "        self.center_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
    "        self.center_emb.weight.data[0] = 0\n",
    "\n",
    "        self.context_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)        \n",
    "        self.context_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
    "        self.context_emb.weight.data[0] = 0\n",
    "\n",
    "        self.positive_sim_mask = make_diag_mask(sentence_len, radius)\n",
    "    \n",
    "    def forward(self, sentences):\n",
    "        \"\"\"sentences - Batch x MaxSentLength - идентификаторы токенов\"\"\"\n",
    "        batch_size = sentences.shape[0]\n",
    "        center_embeddings = self.center_emb(sentences)  # Batch x MaxSentLength x EmbSize\n",
    "\n",
    "        # оценить сходство с настоящими соседними словами\n",
    "        positive_context_embs = self.context_emb(sentences).permute(0, 2, 1)  # Batch x EmbSize x MaxSentLength\n",
    "        positive_sims = torch.bmm(center_embeddings, positive_context_embs)  # Batch x MaxSentLength x MaxSentLength\n",
    "        positive_probs = torch.sigmoid(positive_sims)\n",
    "\n",
    "        # увеличить оценку вероятности встретить эти пары слов вместе\n",
    "        positive_mask = self.positive_sim_mask.to(positive_sims.device)\n",
    "        positive_loss = F.binary_cross_entropy(positive_probs * positive_mask,\n",
    "                                               positive_mask.expand_as(positive_probs))\n",
    "\n",
    "        # выбрать случайные \"отрицательные\" слова\n",
    "        negative_words = torch.randint(1, self.vocab_size,\n",
    "                                       size=(batch_size, self.negative_samples_n),\n",
    "                                       device=sentences.device)  # Batch x NegSamplesN\n",
    "        negative_context_embs = self.context_emb(negative_words).permute(0, 2, 1)  # Batch x EmbSize x NegSamplesN\n",
    "        negative_sims = torch.bmm(center_embeddings, negative_context_embs)  # Batch x MaxSentLength x NegSamplesN\n",
    "        \n",
    "        # уменьшить оценку вероятность встретить эти пары слов вместе\n",
    "        negative_loss = F.binary_cross_entropy_with_logits(negative_sims,\n",
    "                                                           negative_sims.new_zeros(negative_sims.shape))\n",
    "\n",
    "        return positive_loss + negative_loss\n",
    "\n",
    "\n",
    "def no_loss(pred, target):\n",
    "    \"\"\"Фиктивная функция потерь - когда модель сама считает функцию потерь\"\"\"\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:19:33.130307Z",
     "start_time": "2019-10-29T19:19:33.103036Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), 100, MAX_SENTENCE_LEN,\n",
    "                                          radius=5, negative_samples_n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.830221Z",
     "start_time": "2019-10-29T19:19:33.132062Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_val_loss, best_model = train_eval_loop(trainer,\n",
    "                                            train_dataset,\n",
    "                                            test_dataset,\n",
    "                                            no_loss,\n",
    "                                            lr=1e-2,\n",
    "                                            epoch_n=2,\n",
    "                                            batch_size=8,\n",
    "                                            device='cpu',\n",
    "                                            early_stopping_patience=10,\n",
    "                                            max_batches_per_epoch_train=2000,\n",
    "                                            max_batches_per_epoch_val=len(test_dataset),\n",
    "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=1, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.862018Z",
     "start_time": "2019-10-29T19:20:12.832046Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(trainer.state_dict(), f'{BASE_PATH}/models/sgns.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.888270Z",
     "start_time": "2019-10-29T19:20:12.864706Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.load_state_dict(torch.load(f'{BASE_PATH}/models/sgns.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследуем характеристики полученных векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.919904Z",
     "start_time": "2019-10-29T19:20:12.890671Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.942708Z",
     "start_time": "2019-10-29T19:20:12.921619Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings.most_similar('chicken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.965936Z",
     "start_time": "2019-10-29T19:20:12.944423Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings.analogy('cake', 'cacao', 'cheese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:12.991060Z",
     "start_time": "2019-10-29T19:20:12.967532Z"
    }
   },
   "outputs": [],
   "source": [
    "test_words = ['salad', 'fish', 'salmon', 'sauvignon', 'beef', 'pork', 'steak', 'beer', 'cake', 'coffee', 'sausage', 'wine', 'merlot', 'zinfandel', 'trout', 'chardonnay', 'champagne', 'cacao']\n",
    "test_vectors = embeddings.get_vectors(*test_words)\n",
    "print(test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:13.318676Z",
     "start_time": "2019-10-29T19:20:12.996595Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((10, 10))\n",
    "plot_vectors(test_vectors, test_words, how='svd', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение Word2Vec с помощью Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:13.613797Z",
     "start_time": "2019-10-29T19:20:13.321353Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:17.075005Z",
     "start_time": "2019-10-29T19:20:13.615729Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.Word2Vec(sentences=train_tokenized, size=100,\n",
    "                                  window=5, min_count=5, workers=4,\n",
    "                                  sg=1, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:17.109583Z",
     "start_time": "2019-10-29T19:20:17.076599Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec.wv.most_similar('chicken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:17.176357Z",
     "start_time": "2019-10-29T19:20:17.112948Z"
    }
   },
   "outputs": [],
   "source": [
    "gensim_words = [w for w in test_words if w in word2vec.wv.vocab]\n",
    "gensim_vectors = np.stack([word2vec.wv[w] for w in gensim_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:17.428874Z",
     "start_time": "2019-10-29T19:20:17.179311Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((10, 10))\n",
    "plot_vectors(gensim_vectors, test_words, how='svd', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка предобученного Word2Vec\n",
    "\n",
    "Источники готовых векторов:\n",
    "\n",
    "https://rusvectores.org/ru/ - для русского языка\n",
    "\n",
    "https://wikipedia2vec.github.io/wikipedia2vec/pretrained/ - много разных языков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:17.460133Z",
     "start_time": "2019-10-29T19:20:17.430563Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:20:17.980509Z",
     "start_time": "2019-10-29T19:20:17.462239Z"
    }
   },
   "outputs": [],
   "source": [
    "available_models = api.info()['models'].keys()\n",
    "print('\\n'.join(available_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:22:12.649035Z",
     "start_time": "2019-10-29T19:20:17.984118Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pretrained = api.load('word2vec-google-news-300')  # > 1.5 GB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:22:12.651388Z",
     "start_time": "2019-10-29T19:19:29.817Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained.most_similar('cheese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:22:12.652649Z",
     "start_time": "2019-10-29T19:19:29.820Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained.most_similar(positive=['man', 'queen'], negative=['king'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:22:12.653584Z",
     "start_time": "2019-10-29T19:19:29.823Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_words = [w for w in test_words if w in pretrained.vocab]\n",
    "pretrained_vectors = np.stack([pretrained[w] for w in pretrained_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:22:12.654594Z",
     "start_time": "2019-10-29T19:19:29.828Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches((10, 10))\n",
    "plot_vectors(pretrained_vectors, test_words, how='svd', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение\n",
    "\n",
    "* Реализовали Skip Gram Negative Sampling на PyTorch\n",
    "* Обучили на корпусе рецептов\n",
    "    * Сходство слов модель выучила неплохо\n",
    "    * Для аналогий мало данных\n",
    "* Обучили SGNS с помощью библиотеки Gensim\n",
    "* Загрузили веса Word2Vec, полученные с помощью большого корпуса (GoogleNews)\n",
    "    * Списки похожих слов отличаются!\n",
    "    * Аналогии работают"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
