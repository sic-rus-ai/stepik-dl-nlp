{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer, Self-Attention и моделирование языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:04.225361Z",
     "start_time": "2019-11-05T18:27:04.223470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle,\n",
    "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
    "\n",
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
    "# import sys; sys.path.append('./stepik-dl-nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:53.780539Z",
     "start_time": "2019-11-05T18:27:52.444607Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import youtokentome as yttm\n",
    "\n",
    "import dlnlputils\n",
    "from dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n",
    "    save_texts_to_file, LanguageModelDataset, load_war_and_piece_chunks, \\\n",
    "    GreedyGenerator, BeamGenerator\n",
    "from dlnlputils.pipeline import train_eval_loop, init_random_seed\n",
    "from dlnlputils.base import get_params_number\n",
    "\n",
    "init_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка текстов и разбиение на обучающую и тестовую подвыборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:55.233798Z",
     "start_time": "2019-11-05T18:27:55.197616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "all_chunks = load_war_and_piece_chunks('./datasets/war_and_peace.txt')\n",
    "len(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:55.402080Z",
     "start_time": "2019-11-05T18:27:55.379575Z"
    }
   },
   "outputs": [],
   "source": [
    "print(all_chunks[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:55.954154Z",
     "start_time": "2019-11-05T18:27:55.919185Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(all_chunks)\n",
    "\n",
    "TRAIN_SPLIT = int(len(all_chunks) * 0.7)\n",
    "train_texts = all_chunks[:TRAIN_SPLIT]\n",
    "test_texts = all_chunks[TRAIN_SPLIT:]\n",
    "\n",
    "print('Размер обучающей выборки', len(train_texts))\n",
    "print('Размер валидационной выборки', len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация корпуса с помощью BPE\n",
    "\n",
    "BPE - Byte Pair Encoding\n",
    "\n",
    "YouTokenToMe - быстрая реализация BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:56.412237Z",
     "start_time": "2019-11-05T18:27:56.386089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "BPE_MODEL_FILENAME = './models/war_and_peace_bpe.yttm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:56.928780Z",
     "start_time": "2019-11-05T18:27:56.696400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "TRAIN_TEXTS_FILENAME = './datasets/war_and_peace_bpe_train.txt'\n",
    "save_texts_to_file(train_texts, TRAIN_TEXTS_FILENAME)\n",
    "yttm.BPE.train(data=TRAIN_TEXTS_FILENAME, vocab_size=1000, model=BPE_MODEL_FILENAME);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:57.767294Z",
     "start_time": "2019-11-05T18:27:57.731252Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = yttm.BPE(BPE_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:57.897826Z",
     "start_time": "2019-11-05T18:27:57.874631Z"
    }
   },
   "outputs": [],
   "source": [
    "print(' '.join(tokenizer.vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:58.100551Z",
     "start_time": "2019-11-05T18:27:58.075268Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(tokenizer.encode(train_texts[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:59.729717Z",
     "start_time": "2019-11-05T18:27:59.551045Z"
    }
   },
   "outputs": [],
   "source": [
    "train_token_ids = tokenizer.encode(train_texts, bos=True, eos=True)\n",
    "test_token_ids = tokenizer.encode(test_texts, bos=True, eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:00.401753Z",
     "start_time": "2019-11-05T18:27:59.731680Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist([len(sent) for sent in train_token_ids], bins=30)\n",
    "plt.title('Распределение длин фрагментов в токенах')\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:01.153867Z",
     "start_time": "2019-11-05T18:28:00.404320Z"
    }
   },
   "outputs": [],
   "source": [
    "token_counts = np.bincount([token_id for text in train_token_ids for token_id in text])\n",
    "plt.hist(token_counts, bins=100)\n",
    "plt.title('Распределение количества упоминаний токенов')\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:01.204527Z",
     "start_time": "2019-11-05T18:28:01.156884Z"
    }
   },
   "outputs": [],
   "source": [
    "unknown_subwords_in_test = sum(1 for text in test_token_ids for token_id in text if token_id == 1)\n",
    "print('Количество случаев с неизвестными n-граммами символов в валидационной выборке',\n",
    "      unknown_subwords_in_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка датасетов для PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:02.980335Z",
     "start_time": "2019-11-05T18:28:02.938616Z"
    }
   },
   "outputs": [],
   "source": [
    "CHUNK_LENGTH = 80\n",
    "\n",
    "train_dataset = LanguageModelDataset(train_token_ids,\n",
    "                                     chunk_length=CHUNK_LENGTH)\n",
    "test_dataset = LanguageModelDataset(test_token_ids,\n",
    "                                    chunk_length=CHUNK_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:03.085890Z",
     "start_time": "2019-11-05T18:28:03.061652Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:03.260915Z",
     "start_time": "2019-11-05T18:28:03.219571Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(list(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общие классы и функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Маска зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:04.302365Z",
     "start_time": "2019-11-05T18:28:04.244547Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_target_dependency_mask(length):\n",
    "    full_mask = torch.ones(length, length)\n",
    "    ignore_mask = torch.tril(full_mask) < 1\n",
    "    full_mask.masked_fill_(ignore_mask, float('-inf'))\n",
    "    full_mask.masked_fill_(~ignore_mask, 0)\n",
    "    return full_mask\n",
    "\n",
    "make_target_dependency_mask(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование позиции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:05.590059Z",
     "start_time": "2019-11-05T18:28:05.567602Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_positional_encoding(max_length, embedding_size):\n",
    "    time = np.pi * torch.arange(0, max_length).float()\n",
    "    freq_dividers = torch.arange(1, embedding_size // 2 + 1).float()\n",
    "    inputs = time[:, None] / freq_dividers[None, :]\n",
    "    \n",
    "    result = torch.zeros(max_length, embedding_size)\n",
    "    result[:, 0::2] = torch.sin(inputs)\n",
    "    result[:, 1::2] = torch.cos(inputs)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:06.060293Z",
     "start_time": "2019-11-05T18:28:05.708626Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_pos_codes = make_positional_encoding(30, 30)\n",
    "plt.plot(sample_pos_codes[:, ::3].numpy());\n",
    "plt.gcf().set_size_inches((15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основной класс - языковая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:07.079279Z",
     "start_time": "2019-11-05T18:28:07.031056Z"
    }
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, backbone, emb_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "        self.backbone = backbone\n",
    "        self.out = nn.Linear(embedding_size, vocab_size)\n",
    "    \n",
    "    def forward(self, seed_token_ids):\n",
    "        \"\"\"\n",
    "            seed_token_ids - BatchSize x MaxInLen\n",
    "        \"\"\"\n",
    "        batch_size, max_in_length = seed_token_ids.shape\n",
    "\n",
    "        seed_padding_mask = seed_token_ids == 0\n",
    "        dependency_mask = make_target_dependency_mask(max_in_length) \\\n",
    "            .to(seed_token_ids.device)\n",
    "        \n",
    "        seed_embs = self.embeddings(seed_token_ids)  # BatchSize x MaxInLen x EmbSize\n",
    "        pos_codes = make_positional_encoding(max_in_length,\n",
    "                                             self.embedding_size).unsqueeze(0).to(seed_embs.device)\n",
    "        seed_embs = seed_embs + pos_codes\n",
    "        seed_embs = self.emb_dropout(seed_embs)\n",
    "\n",
    "        # BatchSize x TargetLen x EmbSize\n",
    "        target_features = seed_embs\n",
    "        target_features = self.backbone(seed_embs,\n",
    "                                        mask=dependency_mask,\n",
    "                                        src_key_padding_mask=seed_padding_mask)\n",
    "        logits = self.out(target_features)  # BatchSize x TargetLen x VocabSize\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Утилиты для обучения - функция потерь и расписание изменения длины градиентного шага"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:07.797230Z",
     "start_time": "2019-11-05T18:28:07.774142Z"
    }
   },
   "outputs": [],
   "source": [
    "def lm_cross_entropy(pred, target):\n",
    "    \"\"\"\n",
    "    pred - BatchSize x TargetLen x VocabSize\n",
    "    target - BatchSize x TargetLen\n",
    "    \"\"\"\n",
    "    pred_flat = pred.view(-1, pred.shape[-1])  # BatchSize*TargetLen x VocabSize\n",
    "    target_flat = target.view(-1)  # BatchSize*TargetLen\n",
    "    return F.cross_entropy(pred_flat, target_flat, ignore_index=0)\n",
    "\n",
    "\n",
    "def lr_scheduler(optimizer):\n",
    "    return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                      patience=20,\n",
    "                                                      factor=0.5,\n",
    "                                                      verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация Transformer из PyTorch 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:09.401017Z",
     "start_time": "2019-11-05T18:28:09.365637Z"
    }
   },
   "outputs": [],
   "source": [
    "class BatchFirstTransformerEncoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.impl = nn.TransformerEncoder(*args, **kwargs)\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def forward(self, src, *args, **kwargs):\n",
    "        src = src.transpose(0, 1).contiguous()  # MaxInLen  x BatchSize x EmbSize\n",
    "        result = self.impl(src, *args, **kwargs)  # TargetLen x BatchSize x EmbSize\n",
    "        result = result.transpose(0, 1).contiguous()  # BatchSize x TargetLen x EmbSize\n",
    "        return result\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for param in self.impl.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:10.550078Z",
     "start_time": "2019-11-05T18:28:10.425261Z"
    }
   },
   "outputs": [],
   "source": [
    "torch_transf_model = LanguageModel(tokenizer.vocab_size(),\n",
    "                                   256,\n",
    "                                   BatchFirstTransformerEncoder(\n",
    "                                       nn.TransformerEncoderLayer(\n",
    "                                           d_model=256,\n",
    "                                           nhead=16,\n",
    "                                           dim_feedforward=512,\n",
    "                                           dropout=0.1),\n",
    "                                       num_layers=3),\n",
    "                                   emb_dropout=0.1)\n",
    "print('Количество параметров', get_params_number(torch_transf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:58.797642Z",
     "start_time": "2019-11-05T18:28:34.626744Z"
    }
   },
   "outputs": [],
   "source": [
    "(best_val_loss,\n",
    " best_torch_transf_model) = train_eval_loop(torch_transf_model,\n",
    "                                            train_dataset,\n",
    "                                            test_dataset,\n",
    "                                            lm_cross_entropy,\n",
    "                                            lr=2e-3,\n",
    "                                            epoch_n=2000,\n",
    "                                            batch_size=512,\n",
    "                                            device='cuda',\n",
    "                                            early_stopping_patience=50,\n",
    "                                            max_batches_per_epoch_train=1000,\n",
    "                                            max_batches_per_epoch_val=1000,\n",
    "                                            lr_scheduler_ctor=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:00.752077Z",
     "start_time": "2019-11-05T18:29:00.675069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_torch_transf_model.state_dict(), './models/war_and_peace_torch_transf_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:00.844033Z",
     "start_time": "2019-11-05T18:29:00.789023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch_transf_model.load_state_dict(torch.load('./models/war_and_peace_torch_transf_best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация текста с помощью языковой модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Жадная генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:02.366423Z",
     "start_time": "2019-11-05T18:29:02.329495Z"
    }
   },
   "outputs": [],
   "source": [
    "greedy_generator = GreedyGenerator(torch_transf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:03.175509Z",
     "start_time": "2019-11-05T18:29:02.921960Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(greedy_generator('сказала княжна, оглядывая Бона'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:03.497702Z",
     "start_time": "2019-11-05T18:29:03.177598Z"
    }
   },
   "outputs": [],
   "source": [
    "print(greedy_generator('смеялась княжна, оглядывая Наполе'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:03.770265Z",
     "start_time": "2019-11-05T18:29:03.500330Z"
    }
   },
   "outputs": [],
   "source": [
    "print(greedy_generator('сказала княжна, оглядывая Кутуз'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:04.150676Z",
     "start_time": "2019-11-05T18:29:03.773669Z"
    }
   },
   "outputs": [],
   "source": [
    "print(greedy_generator('сказал Кутузов, оглядывая Наполеона'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация с помощью лучевого поиска - Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:08.328662Z",
     "start_time": "2019-11-05T18:29:08.294006Z"
    }
   },
   "outputs": [],
   "source": [
    "beam_generator = BeamGenerator(torch_transf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:10.573399Z",
     "start_time": "2019-11-05T18:29:09.653198Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "beam_gen_variants = beam_generator('сказала княжна, оглядывая Наполе',\n",
    "                                   beamsize=5,\n",
    "                                   return_hypotheses_n=5)\n",
    "\n",
    "for score, pred_txt in beam_gen_variants:\n",
    "    print('****')\n",
    "    print(score)\n",
    "    print(pred_txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:05.050342Z",
     "start_time": "2019-11-05T18:27:05.005Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "beam_gen_variants = beam_generator('сказала княжна, оглядывая Наполе',\n",
    "                                   beamsize=20,\n",
    "                                   return_hypotheses_n=20)\n",
    "\n",
    "for score, pred_txt in beam_gen_variants:\n",
    "    print('****')\n",
    "    print(score)\n",
    "    print(pred_txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:05.051378Z",
     "start_time": "2019-11-05T18:27:05.008Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "beam_gen_variants = beam_generator('сказала княжна, оглядывая Наполе',\n",
    "                                   beamsize=100,\n",
    "                                   return_hypotheses_n=20)\n",
    "\n",
    "for score, pred_txt in beam_gen_variants:\n",
    "    print('****')\n",
    "    print(score)\n",
    "    print(pred_txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собственная реализация MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:13.586871Z",
     "start_time": "2019-11-05T18:29:13.544216Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_multihead_attention(queries, keys, values,\n",
    "                           keys_padding_mask, dependency_mask,\n",
    "                           is_training,\n",
    "                           weights_dropout):\n",
    "    \"\"\"\n",
    "    queries - BatchSize x ValuesLen x HeadN x KeySize\n",
    "    keys - BatchSize x KeysLen x HeadN x KeySize\n",
    "    values - BatchSize x KeysLen x HeadN x ValueSize\n",
    "    keys_padding_mask - BatchSize x KeysLen\n",
    "    dependency_mask - ValuesLen x KeysLen\n",
    "    is_training - bool\n",
    "    weights_dropout - float\n",
    "    \n",
    "    result - tuple of two:\n",
    "        - BatchSize x ValuesLen x HeadN x ValueSize - resulting features\n",
    "        - BatchSize x ValuesLen x KeysLen x HeadN - attention map\n",
    "    \"\"\"\n",
    "\n",
    "    # BatchSize x ValuesLen x KeysLen x HeadN\n",
    "    relevances = torch.einsum('bvhs,bkhs->bvkh', (queries, keys))\n",
    "    \n",
    "    # замаскировать элементы, выходящие за длины последовательностей ключей\n",
    "    padding_mask_expanded = keys_padding_mask[:, None, :, None].expand_as(relevances)\n",
    "    relevances.masked_fill_(padding_mask_expanded, float('-inf'))\n",
    "    \n",
    "    # замаскировать пары <выходная позиция, входная позиция>\n",
    "    relevances = relevances + dependency_mask[None, :, :, None].expand_as(relevances)\n",
    "    \n",
    "    normed_rels = F.softmax(relevances, dim=2)    \n",
    "    normed_rels = F.dropout(normed_rels, weights_dropout, is_training)\n",
    "    \n",
    "    # BatchSize x ValuesLen x KeysLen x HeadN x 1\n",
    "    normed_rels_expanded = normed_rels.unsqueeze(-1)\n",
    "    \n",
    "    # BatchSize x 1 x KeysLen x HeadN x ValueSize\n",
    "    values_expanded = values.unsqueeze(1)\n",
    "    \n",
    "    # BatchSize x ValuesLen x KeysLen x HeadN x ValueSize\n",
    "    weighted_values = normed_rels_expanded * values_expanded\n",
    "    result = weighted_values.sum(2)  # BatchSize x ValuesLen x HeadN x ValueSize\n",
    "    \n",
    "    return result, normed_rels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention - это Attention, в котором ключи, значения и запросы вычисляются из элементов одной и той же последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:14.090216Z",
     "start_time": "2019-11-05T18:29:14.064361Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyMultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, model_size, n_heads, dropout=0):\n",
    "        super().__init__()\n",
    "        assert model_size % n_heads == 0, 'Размерность модели должна делиться нацело на количество голов'\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.queries_proj = nn.Linear(model_size, model_size)\n",
    "        self.keys_proj = nn.Linear(model_size, model_size)\n",
    "        self.values_proj = nn.Linear(model_size, model_size)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.last_attention_map = None\n",
    "    \n",
    "    def forward(self, sequence, padding_mask, dependency_mask):\n",
    "        \"\"\"\n",
    "        sequence - BatchSize x Len x ModelSize\n",
    "        padding_mask - BatchSize x Len\n",
    "        dependency_mask - Len x Len\n",
    "        \n",
    "        result - BatchSize x Len x ModelSize\n",
    "        \"\"\"\n",
    "        batch_size, max_len, model_size = sequence.shape\n",
    "        \n",
    "        queries_flat = self.queries_proj(sequence)  # BatchSize x Len x ModelSize\n",
    "        queries = queries_flat.view(batch_size, max_len, self.n_heads, -1)\n",
    "        \n",
    "        keys_flat = self.keys_proj(sequence)  # BatchSize x Len x ModelSize\n",
    "        keys = keys_flat.view(batch_size, max_len, self.n_heads, -1)\n",
    "        \n",
    "        values_flat = self.values_proj(sequence)  # BatchSize x Len x ModelSize\n",
    "        values = values_flat.view(batch_size, max_len, self.n_heads, -1)\n",
    "        \n",
    "        # BatchSize x Len x HeadsN x ValueSize\n",
    "        result, att_map = my_multihead_attention(queries, keys, values,\n",
    "                                                 padding_mask, dependency_mask,\n",
    "                                                 self.training, self.dropout)\n",
    "        result_flat = result.view(batch_size, max_len, model_size)\n",
    "        \n",
    "        self.last_attention_map = att_map.detach()\n",
    "\n",
    "        return result_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Один слой трансформера - Self-Attention, Feed-Forward, skip-connections, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:15.069352Z",
     "start_time": "2019-11-05T18:29:15.028453Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_size, n_heads, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attention = MyMultiheadSelfAttention(model_size,\n",
    "                                                       n_heads,\n",
    "                                                       dropout=dropout)\n",
    "        self.first_dropout = nn.Dropout(dropout)\n",
    "        self.first_norm = nn.LayerNorm(model_size)\n",
    "        \n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(model_size, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, model_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.second_norm = nn.LayerNorm(model_size)\n",
    "    \n",
    "    def forward(self, sequence, padding_mask, dependency_mask):\n",
    "        att_features = self.self_attention(sequence, padding_mask, dependency_mask)\n",
    "\n",
    "        sequence = sequence + self.first_dropout(att_features)\n",
    "        sequence = self.first_norm(sequence)\n",
    "        \n",
    "        sequence = sequence + self.feedforward(sequence)\n",
    "        sequence = self.second_norm(sequence)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Энкодер Трансформера - стопка из нескольких слоёв"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:15.728916Z",
     "start_time": "2019-11-05T18:29:15.680640Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyTransformerEncoder(nn.Module):\n",
    "    def __init__(self, n_layers, **layer_kwargs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MyTransformerEncoderLayer(**layer_kwargs)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, sequence, mask, src_key_padding_mask):\n",
    "        for layer in self.layers:\n",
    "            sequence = layer(sequence, src_key_padding_mask, mask)\n",
    "        return sequence\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем обучить языковую модель с нашим Трансформером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:18.531896Z",
     "start_time": "2019-11-05T18:29:18.460196Z"
    }
   },
   "outputs": [],
   "source": [
    "my_transf_model = LanguageModel(tokenizer.vocab_size(),\n",
    "                                256,\n",
    "                                MyTransformerEncoder(\n",
    "                                    n_layers=3,\n",
    "                                    model_size=256,\n",
    "                                    n_heads=16,\n",
    "                                    dim_feedforward=512,\n",
    "                                    dropout=0.1),\n",
    "                                emb_dropout=0.1)\n",
    "print('Количество параметров', get_params_number(my_transf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:30.000482Z",
     "start_time": "2019-11-05T18:29:24.291741Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(best_val_loss,\n",
    " best_my_transf_model) = train_eval_loop(my_transf_model,\n",
    "                                         train_dataset,\n",
    "                                         test_dataset,\n",
    "                                         lm_cross_entropy,\n",
    "                                         lr=2e-3,\n",
    "                                         epoch_n=2000,\n",
    "                                         batch_size=512,\n",
    "                                         device='cuda',\n",
    "                                         early_stopping_patience=50,\n",
    "                                         max_batches_per_epoch_train=1000,\n",
    "                                         max_batches_per_epoch_val=1000,\n",
    "                                         lr_scheduler_ctor=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:30.732005Z",
     "start_time": "2019-11-05T18:29:30.686738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_my_transf_model.state_dict(), './models/war_and_peace_my_transf_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:30.853057Z",
     "start_time": "2019-11-05T18:29:30.811442Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "my_transf_model.load_state_dict(torch.load('./models/war_and_peace_my_transf_best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наша реализация - жадная генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:31.862429Z",
     "start_time": "2019-11-05T18:29:31.841831Z"
    }
   },
   "outputs": [],
   "source": [
    "my_greedy_generator = GreedyGenerator(my_transf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:32.263263Z",
     "start_time": "2019-11-05T18:29:31.988891Z"
    }
   },
   "outputs": [],
   "source": [
    "my_greedy_generator('сказала княжна, оглядывая Андре')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация карт внимания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:33.644923Z",
     "start_time": "2019-11-05T18:29:33.614615Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_attention_maps(model, input_string, tokenizer, device='cuda', max_heads=2, figsize=(16, 10)):\n",
    "    device = torch.device(device)\n",
    "\n",
    "    token_ids = tokenizer.encode([input_string])[0]\n",
    "\n",
    "    token_strs = [tokenizer.id_to_subword(i) for i in token_ids]\n",
    "    in_len = len(token_ids)\n",
    "    ticks = np.arange(0, in_len)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    in_batch = torch.tensor(token_ids).unsqueeze(0).to(device)\n",
    "    model(in_batch)\n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, MyMultiheadSelfAttention):\n",
    "            cur_last_attention_map = module.last_attention_map[0].cpu().numpy()\n",
    "            n_heads = cur_last_attention_map.shape[-1]\n",
    "            n_heads_to_vis = min(n_heads, max_heads)\n",
    "\n",
    "            fig, axes = plt.subplots(1, n_heads_to_vis)\n",
    "            fig.set_size_inches(figsize)\n",
    "            for head_i in range(n_heads_to_vis):\n",
    "                ax = axes[head_i]\n",
    "                ax.imshow(cur_last_attention_map[..., head_i])\n",
    "\n",
    "                ax.set_yticks(ticks)\n",
    "                ax.set_ylim(bottom=in_len - 0.5, top=-0.5)\n",
    "                ax.set_yticklabels(token_strs)\n",
    "\n",
    "                ax.set_xticks(ticks)\n",
    "                ax.set_xticklabels(token_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:34.935250Z",
     "start_time": "2019-11-05T18:29:33.745970Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_attention_maps(my_transf_model, 'сказал Кутузов, оглядывая Бонапарта', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
